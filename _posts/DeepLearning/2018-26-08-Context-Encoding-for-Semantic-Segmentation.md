---
layout: post
title: Context Encoding for Semantic Segmentation
date: 2018-06-07
categories: DeepLearning 
tags: Deeplearning MachineLearning
mathjax: true
author: Prince
---

* content
{:toc}
![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180621160239.png)

文章地址[Context Encoding for Semantic Segmentation](https://arxiv.org/abs/1803.08904)

语义分割的目标是逐像素分类，对图像中的某一个点进行分类，早期的语义分割任务基于阈值法（自适应阈值OTSU也算是阈值法）效果不好，自Jonathan Long等人提出FCN网络，通过使用没有FC层等需要固定输入大小的网络结构，采用一个几乎通体都是卷积层的网络结构学习输入图像，最后输出一个与输入图像大小一致（或比输入图像稍小）的特征图，最后通过1x1的卷积层来对像素点进行分类。这种FCN结构对整个语义分割任务来说，是一个质的飞越，神经网络也在语义分割这个任务上达到了最好的结果。

不过深度学习用在语义分割上还存在着不少难以解决的问题，如多尺度图像问题，或是上下文信息的利用问题等，后来的U-Net，Deeplab，RefineNet，PSPNet都各自使用了不同的结构与方法来解决这些问题，本文的立足点是利用Context上下文信息增强语义分割效果，主要贡献有以下几点：

1. 结合传统“字典学习”的方法提出了一种使用上下文信息的结构：Context Encoding Module(CEM)
2. 基于CEM设计了一种网络结构EncNet用来使用上下文信息增强语义分割效果
3. 在COCO上达到了state of art





## 语义分割的难点及解决方法

语义分割的难点可以总结为以下几点：

### 1. 多尺度问题

一般自然图像的分割问题存在所谓的“景深”问题，即一个物体在远处会比较小，在近处比较大，因此，通过网络中设计适应于这种multiscale的结构能够提高分割问题的小物体预测的准确率，Mult-scale的方法通过Deeplab中的多尺度膨胀卷积，PSPNet中的多种尺度的ROIPooling或是一些网络结构通过不同大小的特征图fusion来预测最终的标签等解决办法。

### 2. 上下文运用

上下文信息，换种说法可以说是目标周围的环境信息或者是全局的环境信息，举个简单的例子，在室内语义分割任务中，常需要把“床”，“桌子”，“椅子”等类别分割出来，一般的分割网络直接学习这些类别，所利用的信息比较局部，比如提取该像素周围的纹理信息等，但是如果在全局信息已经给出，“该场景是一个卧室”，那么床，窗户，窗帘等常出现在卧室的类别的概率应该被大大提升，其他如天空，飞机之类的类别概率应该被抑制。另外，一些常出现在一起的物体，如火车和铁路，床和地板等物体的关联性应该被表现出来等，这些都属于上下文信息。

**解决办法**

之前的网络通过增大感受野的方法来体现上下文信息，如通过Atrous Convolution在不减小分辨率的情况下，增大感受野，也有一些网络使用Global Average Pooling的方法来得到全局信息（比如PSPNet）等，这些方法试图通过增大感受野，能更好的利用上下文，但是终归这些方法在训练过程中是无监督的，是否能够work，全靠网络训练过程中的梯度回传，没有明确的监督，使这种方法不见得非常鲁棒。

基于以上上下文信息的利用，作者提出一个问题：

> Question: is capturing contextual information the same as increasing the receptive ﬁeld size? 
>
> 是否上下文环境的利用就等同于使用更大的感受野来进行预测？

我们的分析来看，显然不是的，增大感受野虽然能够起到使用context的作用，但是毕竟是无监督的，网络是否真正使用到了上下文信息，我们也无从得知，因此作者提出了更加明确利用上下文环境的结构：CEM

## 传统字典学习与场景分类

在早些时候，要分类一个场景是一个卧室还是飞机场，一般是通过“字典学习”的方法来实现的，比如需要分类一个卧室，我先学习这个场景中是否有床，窗户，地板等类别，预测出这些类别的概率，再通过一个类似于FC的结构将这些概率加权以后转换成场景的概率；可以认为是在字典学习的基础上增加了一个FC层来学习场景分类。而是否有床，窗户等，一般是通过不同卷积核的滤波器，比较小范围的相似度来实现的。

## CEM结构与Encoding Layer

### CEM结构

Context Encoding Module是一种利用传统字典学习的方法来解决context运用的问题，整体结构如下图

- 首先将任一大小的图像通过特征提取网络，得到一个[HxWxC]的特征图
- 将这个特征图通过一个Encoding Layer映射到一个固定长度的编码（可以认为是embeding）
- 分支结构
  - 对这个把这个编码一个分支通过FC来预测图中是否存在某些类别，比如这个图中有床，床那个分类就是1，而不学习这个床的位置信息，只学习床是否存在。
  - 在主干分支，将刚才固定长度的编码通过一个FC映射到C通道，C即刚才特征提取网络输出的通道数，经过sigmoid后直接乘到特征图上做通道维度的attention
- 把最后的特征图放大，常规操作如softmax等，最后得到语义分割结果

![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180621160239.png)

这种方法通过预测物体在输入图片的存在性来利用上下文信息，其优点有以下两点：

- **不需要额外的标记**：一些网络也试图通过学习context来增强语义分割，但是可能会用到一些额外标记，比如这张图片是“卧室”这样的标记，给标记带来额外的负担。
- **小物体敏感：**语义分割中小物体不好预测出来部分原因是大物体所占像素更多，因此在最终的target上也占了更大的面积，所以反向传播就会给大物体更多的梯度，但是在本文的架构中，大物体和小物体在分类上的贡献是一样的，不会因为所占像素的多少影响梯度的传播，因此对小物体效果不好也有一些改善。
- **防止过拟合**：给在特征图层面网络多一些限制（必须预测出物体存在），也算是一种正则。

### Encoding Layer

整体结构的思想还是比较简单的，学习各种类别的存在性来对通道维度进行attention，但这其中的具体结构还是需要了解一下，比如，CEM是如何将任一大小的特征图映射到固定维度。一拿到这个问题有几个比较naive的解法：

- 把所有的图都padding到统一大小
- 对特征图进行类似ROIPooling，或者Spatial Pyramid Pooling等结构

第一种方法不可取，由于一些图像大小差距比较大，具体padidng成多大的也不好确定，要是使用resize的话会导致图像的变形，而作者没有使用第二种方法，具体原因可能是ROIPooling等方法虽然可以将图像encoding到固定长度，但是没办法让每个类别的特征学习的更加可分。作者使用了[Deep TEN: Texture Encoding Network](https://arxiv.org/abs/1612.02844)这篇文章中的Encoding Layer，将一个任意大小的图像encoding成为固定大小的编码，同时学习的特征具有可分性。



下图是Encoding Layer的一个说明：

![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180608213755.png)

首先输入时一个[HxWxD]的特征图，然后构造K个中心（$c_0, c_1, ... c_{K-1}$）,现在特征图上的每个位置都可以看做是一个D维的向量$x_i$，每个中心也是D维向量。现在对于某个中心$c_k$，我们把每个点$x_i$对$c_k$的相对坐标都计算出来，作为这个点在这个中心上的投影。把整张图上的所有$x_i$的投影都加起来，就得到了这张图在这个中心上的投影。

然而我们希望里这个中心越远的x，对这个投影计算的影响越小，这个简单，加一个权值就好了，于是有了以下公式：
$$
r_{ik} = x_i - c_k
$$

$$
e_k = \sum_{i=1}^N e_{ik} =  \sum_{i=1}^N a_{ik} r_{ik}
$$

$$
a_{ik} = \frac{\exp(-\beta\|r_{ik}\|^2)}{\sum_{j=1}^K(-\beta\|r_{ij}\|^2)}
$$

最后拿到$e_k$，其中N是x的总数，一般等于HxW。注意到$a_{ik}$其实是利用softmax来对于中心店的距离，距离($ x_i - c_k$)越大，权值越小，最后进行一个加权求和，得到整张图像在这个中心上的映射。

接下来求出该特征图在每个中心上的映射，得到这张图在k个d维的向量，在文章中，作者在利用BN和relu处理一下这个k个d维的特征（以下公式中的$\phi$代表BN+Relu），然后再加起来，作为整张图像的encoding:
$$
e = \sum_{k=1}^{K} \phi(e_k)
$$


## EncNet

![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180608211651.png)

经过以上的结构，我们知道，不管多大的特征图都会被映射到固定大小的维度，那么现在就可以在这个基础上做文章了：EncNet的整体架构是ResNet，在经过Stage2后假设我们得到的特征图是d维，按照总结构图的说法，经过一个FC用来做一个多标签分类，使用CrossEntropy作为loss函数，也就是所谓的**SE-loss**，判断图中是否存在某些哪些类别，并且使用另外一个FC映射到C（注意EncNet图中的C就是上文所说的d）经过sigmoid以后，做一个通道维度（Channelwise）的乘法，这样就可以作为attention使用。

作者在设计网络的时候，只在最后两个block使用了这种结构，即上图中的`SE-loss1`和`SE-loss2`，第一个只是单纯的通道attention，最后一个SE-loss则意义更加明确：由于最后一层的每个通道代表一个类别，**因此最后层的通道attention即改类别的存在概率**。而Seg-loss就是普通的通道维度的CE或是分割常用的Dice Loss。

## 实验

文中做了与ResNet的丢笔试眼，并使用消融实验给出了各个部分的提升：

![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180621161406.png)

可以看到，Encoding的提升比较大（约5个百分点Acc和6.6的mIoU），加入SE-loss以后也提升了一些。最后放一张效果图，主要与FCN作比较，可以看出的是，EncNet对Context更加敏感，在预测上下文环境时比FCN有明显提高：

![](http://princepicbed.oss-cn-beijing.aliyuncs.com/blog_20180621161711.png)

实验代码可以在[这里](https://github.com/zhanghang1989/PyTorch-Encoding)找到

## 总结

EncNet主要通过类似传统CV任务中的字典学习，尝试理解输入图像中存在哪些物体，进而来利用上下文环境信息，在使用Context时，不同于之前网络只使用了更大的感受野，EncNet通过预测物体的存在性来使用上下文环境的方式更加合理，其优点在于能够更好的让小物体凸现出来等，对于物体的位置信息，如`火车`在`铁路`上面之类的位置信息，没有很好的运用，可能还有改进之处，另一方面，Deep TEN中对特征图的Encoding也可以说是很有自己的见解，值得学习。



