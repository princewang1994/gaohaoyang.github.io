---
layout: post
title:  Decouple Networks
date:   2018-05-05
categories: DeepLearning 
tags: DeepLearning MachineLearning
mathjax: true
author: Prince
---

* content
{:toc}

abstract，TOC和abstract空一行，abstract和标题1之前空4行，abstract中可以加入图片，会显示在首页




## 0. 前言

本文记录了自己阅读[Decouple Network](https://arxiv.org/abs/1804.08071)的一些想法，Decouple Network是CVPR2018的Spotlight论文，有着很不错的数学功底，该文章的作者Weiyang Liu，也曾经在NIPS上发表过于这篇文章很相似的Hyper Sphere Learning，这篇文章主要也是延续了NIPS上这篇文章的主要思想，在此基础上做了一些扩展。

## 1. Motivation

​	传统的CNN核心操作是卷积操作，这种卷积操作本质上是输入(x)的某一个patch与卷积核w的内积，这里我们把这两个变量分别设为x和w，我们知道，卷积神经网络最后是预测某一个类别的概率，也就是特征图的值，比如在ImageNet的分类上，ResNet的特征图最后被缩到很小，每一个点上，每个通道都表示了不同类别的预测概率。但是卷积操作本身来说，本质上是内积计算$||w|| \cdot ||x||cos(\theta_{(w, x)})$，最主要的功能是衡量x与w之间的相似性，两个向量越相似（这时候w可以看做是匹配模式）我们可以把内积操作解耦成两个方面：类内差异和类间差异，接下来解释这两个差异的意思：

​	如下图表示使用一个CNN训练神经网络时，将最后一层的特征压缩到两维以后的在二维空间上可视化图像，可以看出来，这张图的中心坐标是(0,0)每一个数字的特征都聚集在一个特殊的区域，这时候，两个类不同的时候，向量的角度大小越大，这两个类越不像，若是属于同一个类别，那么向量长度越小，表示网络越不确定应该把他分成那个类别，越往外，置信度越高。假设这个矩阵是shape是[bs, 2]，后面乘以一个[2, 10]的权值矩阵w，w可以看做是10个2维向量分别于这个二维特征做内积，得到10个概率。比如8对应w与x相乘，得到该特征是8的概率，由此可见，特征向量x与w向量相乘值很大，原因可能有很多种：

1. 两个向量的角度很小，Cosine很大（也就是很像同一个数字）

2. 两个向量角度虽然不是很小，Cosine不算很大，但是向量长度（置信度）很大

   ![](http://oodo7tmt3.bkt.clouddn.com/blog_20180528213214.png)

   由此，我们很难区分输出值高是因为角度的影响还是置信度影响，在本文中，作者由这点为立足点，将卷积操作解耦为$h(||w||, ||x||) \cdot g(\theta_{(w,x)})$，其中h和g分别是：

- $h(\cdot)$: 量级函数(Magnitude function)，用来衡量类内的variance，也可以理解为某一个类别的置信度
- $g(\cdot)$: 角度激活函数(Augular activation function)，类间区分度，代表两个向量的区别程度，越大越像。

很容易看出，基础的卷积操作，是解耦卷积操作的一种特殊形式，即$h(||w||, ||x||) = ||x|| \cdot ||w||$，$ g(\theta_{(w,x)}) = cos<w, x>$，但是通过修改h和g两个函数，对网络进行解耦。

本文的贡献点：

- 提出一种显式解耦操作的可学习网络结构DCNet
- 通过设计解耦操作的尺度函数和角度函数可以使解耦卷积适应不同的任务
- 证明DCNet比普通CNN更容易收敛

## 2. 网络结构

DCNet本身的结构与传统CNN一致，区别在于卷积操作的运算过程，DCNet的主要优点在于：

1. 可以用一些非线性的函数，而不是固定的内积，将原本的内积操作解耦成$h(w, x) \cdot g(\theta_{(w, x)})$
2. 使用bounded magnitude function可以有bn的作用，使网络收敛更快
3. 架构稳定性较高，可以很好的应对对抗样本攻击
4. 解耦操作非常灵活，并可以直接在已有网络的基础上（比如VGG）修改，（甚至可以使用其他网络的预训练参数，后文会提到）

### 2.1 量级函数（Magnitude function）

量级函数代表了类内的间距，一般我们认为，这个间距应该只取决于x的范数，而不应该和w的范数有关系，w的范数可以认为是这个类别的重要程度。因此会看到本文设计的量级函数，都是把$||w||$设置为1，使h函数只关于$||x||$，接下来介绍本文提到的集中量级函数，量级函数分为两种：受限和不受限，受限函数在$||x||$超过一定大小的时候会被限制，起到一种限制w大小的作用，这样可以限制w的大小，对梯度传播有益。

### 2.2 受限量级函数（Bounded）

**Hyperspherical**

超球量级函数其实就是把所有的w和x都归一化到一个固定长度$\alpha$，来抵消w对类内方差的影响，具体的做法就是直接令$h(w, x)=\alpha$

$$
h(||w||, ||x||) = \alpha
$$

通过修改f函数:

$$
f_d(w, x) = \alpha \cdot g(\theta_{(w, x)})
$$
但是这种函数有一个问题，$h(||w||, ||x||) = 0$是一个很强的假设，在网络学习的过程中，假设太强会导致优化困难，例如在ResNet的中间层加入许多中间层的softmax和CrossEntropy回归，这种强约束会让网络很难收敛，于是有了BallConv改进。

**BallConv**

BallConv的改进其实是利用一个操作半径$\rho$，在HyperSphere的基础上，如果$||x|| < \rho$，那么h(x)直接按x线性增长，如果超过$\rho$，再将h(x)限制在$\rho$的范围内，具体的，BallConv的函数定义为：

$$
f_d(w, x) = \alpha \cdot \frac{\min(||x||, \rho)}{\rho} \cdot g(\theta_{(w,x)})
$$

对于$||x|| < \rho$的情况，$f(w,x ) = \frac{\alpha}{\rho} \cdot ||x||$，线性增长，否则，与HyperSphere函数行为一致。个人觉得这里有点weight clip的意思。

**Tanh**

BallConv有一个问题就是，在$||x|| = \rho$的点是不平滑的，已经有文献证明，这种不连续会导致优化困难，因此在这里作者提出了平滑版的BallConv，也就是Tanh magnitude function，平滑版的具体公式如下：


$$
f_d(w, x) = \alpha \ tanh(\frac{||x||}{\rho}) \cdot g(\theta_{(w,x)})
$$

随着x长度增大，$tanh(||x|| / \rho)$的增长逐渐变缓，由于是对数函数，$||x||$增长到一定程度以后基本就是一个常数，这个Tanh函数既有ballconv的优点，也因为有平滑，能够很好的收敛。


### 非受限函数（Unbounded）

非受限函数

**Linear**
最简单的线性函数，计算方法如下，$\alpha$为线性斜率：
$$
f_d(w, x) = \alpha ||x|| \cdot g(\theta_{(w,x)})
$$
**Segmented**
![](http://oodo7tmt3.bkt.clouddn.com/blog_20180529111402.png)

**Logarithm**
$$
f_d(w, x) = \alpha \log{(1 + ||x||)} \cdot g(\theta_{(w,x)})
$$
**Mixed**
$$
f_d(w, x) = (\alpha ||x|| + \beta \log{(1 + ||x||)}) \cdot g(\theta_{(w,x)})
$$

### 角度激活函数(Angular Activation Function)

角度激活函数的作用是给g函数一些更加灵活的设计，但总体还是要遵从，角度越小，值越大的整体特性，只是增长方式有所不同，这里列出几个Angular函数：

**Linear angular**：
$$
g(\theta_{(w,x)}) = − \frac{2}{\pi} \theta_{(w,x)} + 1,
$$

**Cosine angular**(原始卷积中使用的函数)：
$$
g(\theta_{(w,x)}) = \cos{(w, x)}
$$


**Sigmoid angular activation**(公式很复杂)：
$$
g(\theta_{(w,x)}) = \frac{1 + \exp{(-\frac{\pi}{2k})}}{1 - \exp{(-\frac{\pi}{2k})}}   \cdot   \frac{1 - \exp{(\frac{\theta_{(w,x)}}{k}-\frac{\pi}{2k})}}{1 + \exp{(\frac{\theta_{(w,x)}}{k}-\frac{\pi}{2k})}}
$$

## 3. 解耦算子的可视化

​	如下图是解耦算子的可视化，主要用来理解量级函数的，除了左上角第一个base convolution，剩余的都设置$||w||=1$，这是因为一般认为$||w||$与x长度相乘对分类没有直接相关，而是x长度与inner-class variance有关系，下图中红色代表最终相乘的向量，绿色代表原始向量长度，圆形半径即**操作半径**，一些首先量级函数在$||x||$超过半径的时候会有很大的限制（如Hyperball，Tanh）

![](http://oodo7tmt3.bkt.clouddn.com/blog_20180528213237.png)



### 权重加入解耦算子

- 线性
- 非线性


## 4. 其他细节

**可学习解耦算子**

为了增加网络的拟合能力，也避免太多参数要调(解耦算子增加了非常多参数如半径，$\alpha$等等)，作者也实验了将这些参数变为可通过SGD来学习的，但是太多参数可学习导致网络可能需要大量的数据来达到良好的泛化为了达到trade off可学习与数据量的问题，本文得出的结论是只使操作半径$\rho$可学习

**权重投影（Weight Projection）**

w的范数会影响w的梯度，范数越大，梯度越小，因此如果范数过大，很容易造成梯度消失问题，为了解决这个问题，每个一段时间就要对w做norm，这里的具体做法是权值投影：
W = s* w^把w的范数置为s，但是由于权值投影会很大程度上影响forwardpass，所以只在反向传播的时候使用

**DCNet的统一性**

解耦算子具有很强的统一性，也可以说他是目前很多方法的超集，比如Conv+Relu的经典结构可以被表示为解耦算子$||w|| \cdot ||x|| \cdot \max{(0, \cos(\theta))}$，bn也可以简单的认为是$||w|| \cdot ||x|| \cdot \cos(\theta)$只是函数不一样，结构的方法有很好的统一性

## 5. 实验

## 6. 总结